{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lab50_text_tokenizer_Eng_Korean.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtajeong/AI_Cluster/blob/main/lab50_text_tokenizer_Eng_Korean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0qTQjjQTrdm"
      },
      "source": [
        "\n",
        "# Text Classification: Survey\n",
        "- https://github.com/kk7nc/Text_Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Baohdf6fOOI"
      },
      "source": [
        "## 1. Feature extraction\n",
        "- text cleaning an dpreprocessing\n",
        "- tokenization\n",
        "- stop words\n",
        "- noise removal\n",
        "- spelling correction\n",
        "- stemming\n",
        "- lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCue_kEvgOpf",
        "outputId": "1d29b6ff-f0fa-4c60-a8a3-620f75a79f19"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8fcX8OaTrdo"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JJ_pe-Vf2Ow",
        "outputId": "cd6ad3d8-c3dd-4e7f-f70e-e9f88cf4bc4f"
      },
      "source": [
        "# tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCIFbCTiXw7X",
        "outputId": "53063dee-21d8-4aeb-cf8b-66fea818b202"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"After sleeping for four hours, he decided to sleep for another four\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['After', 'sleeping', 'for', 'four', 'hours', ',', 'he', 'decided', 'to', 'sleep', 'for', 'another', 'four']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V2WmYvxgflI",
        "outputId": "f62e13d9-4c29-40d8-a68f-5aaeaad868db"
      },
      "source": [
        "# stop words\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLSSWOhqYDio",
        "outputId": "970db698-6cfe-47b1-fb61-b51fbda52fb0"
      },
      "source": [
        "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T04oQcZYhEOB",
        "outputId": "d8f39393-10f8-4bf0-fdd3-a6eadfb982f7"
      },
      "source": [
        "# captitalization\n",
        "text = \"The United States of America (USA) or America, is a federal republic composed of 50 states\"\n",
        "print(text)\n",
        "print(text.lower())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The United States of America (USA) or America, is a federal republic composed of 50 states\n",
            "the united states of america (usa) or america, is a federal republic composed of 50 states\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XW-yMZO7hKV-",
        "outputId": "c5bcf9ce-c108-41d4-db89-938cf17449e8"
      },
      "source": [
        "# noise removal\n",
        "import re\n",
        "def text_cleaner(text):\n",
        "    rules = [\n",
        "        {r'>\\s+': u'>'},  # remove spaces after a tag opens or closes\n",
        "        {r'\\s+': u' '},  # replace consecutive spaces\n",
        "        {r'\\s*<br\\s*/?>\\s*': u'\\n'},  # newline after a <br>\n",
        "        {r'</(div)\\s*>\\s*': u'\\n'},  # newline after </p> and </div> and <h1/>...\n",
        "        {r'</(p|h\\d)\\s*>\\s*': u'\\n\\n'},  # newline after </p> and </div> and <h1/>...\n",
        "        {r'<head>.*<\\s*(/head|body)[^>]*>': u''},  # remove <head> to </head>\n",
        "        {r'<a\\s+href=\"([^\"]+)\"[^>]*>.*</a>': r'\\1'},  # show links instead of texts\n",
        "        {r'[ \\t]*<[^<]*?/?>': u''},  # remove remaining tags\n",
        "        {r'^\\s+': u''}  # remove spaces at the beginning\n",
        "    ]\n",
        "    for rule in rules:\n",
        "        for (k, v) in rule.items():\n",
        "            regex = re.compile(k)\n",
        "            text = regex.sub(v, text)\n",
        "    text = text.rstrip()\n",
        "    return text.lower()\n",
        "\n",
        "text_cleaner('  The   United States of America is a \\\n",
        "              federal republic composed of 50 states.  ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the united states of america is a federal republic composed of 50 states.'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s36QYm9DjQnp",
        "outputId": "7b55e44b-5b4b-4306-cd70-0f9d0322ad6b"
      },
      "source": [
        "!pip install autocorrect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.5.0.tar.gz (622 kB)\n",
            "\u001b[K     |████████████████████████████████| 622 kB 5.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.5.0-py3-none-any.whl size=621851 sha256=4d6a858a46a6ec802ce89814f3d51b0eabb1f563841485bf90311c9cb417636f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/8e/bd/f6fd900a056a031bf710a00bca338d86f43b83f0c25ab5242f\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enQi-y9di39h",
        "outputId": "ae6b29d9-b63f-409e-a498-aa46321e2de9"
      },
      "source": [
        "# spelling correction\n",
        "from autocorrect import spell\n",
        "spell('caaaar'), spell('mussage'), spell('survice'), spell('hte')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
            "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('aaaaaa', 'message', 'service', 'the')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdDkm6Dkiu9v",
        "outputId": "241dbcda-9fcf-4c3b-ceb5-33196f3dc715"
      },
      "source": [
        "# stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "ps = PorterStemmer()\n",
        "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\", \"studying\", \"taxies\"]\n",
        "for w in example_words:\n",
        "    print(ps.stem(w))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python\n",
            "python\n",
            "python\n",
            "python\n",
            "pythonli\n",
            "studi\n",
            "taxi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPW98MtCkVna",
        "outputId": "958c02b7-2a91-4c4d-d080-9a711f35f19f"
      },
      "source": [
        "# lemmatization: eliminating redundant prefix or suffix of a word and extract the base word (lemma)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"cats\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "cat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIr0eUfGasT3"
      },
      "source": [
        "# 한글"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Avn9WrZIa5R3",
        "outputId": "8937bf05-3fd3-4f80-b92b-78446f625193"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 121 kB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXpFHYvBarro",
        "outputId": "a797b14d-8ed1-4bba-fa50-96f250298bea"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "\n",
        "print(okt.morphs(\"우리가 이 과제를 잘 할 수 있을까?\"))\n",
        "print(okt.pos(\"우리가 이 과제를 잘 할 수 있을까?\", norm=True, stem=True))\n",
        "print(okt.nouns(\"우리가 이 과제를 잘 할 수 있을까?\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['우리', '가', '이', '과제', '를', '잘', '할', '수', '있을까', '?']\n",
            "[('우리', 'Noun'), ('가', 'Josa'), ('이', 'Noun'), ('과제', 'Noun'), ('를', 'Josa'), ('자다', 'Verb'), ('하다', 'Verb'), ('수', 'Noun'), ('있다', 'Adjective'), ('?', 'Punctuation')]\n",
            "['우리', '이', '과제', '수']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrkM2qLybuEH"
      },
      "source": [
        "- 1) morphs : 형태소 추출\n",
        "- 2) pos : 품사 태깅(Part-of-speech tagging)\n",
        "- 3) nouns : 명사 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6LWKSImbVwX",
        "outputId": "e37f91c0-48b4-437e-e979-562245aa5cee"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "okt=Okt()\n",
        "word_tags = okt.pos(\"우리가 이 과제를 잘 할 수 있을까?\", norm=True, stem=True)\n",
        "print(word_tags)\n",
        "stop_words = [word[0] for word in word_tags if word[1]==\"Josa\"]\n",
        "print (stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('우리', 'Noun'), ('가', 'Josa'), ('이', 'Noun'), ('과제', 'Noun'), ('를', 'Josa'), ('자다', 'Verb'), ('하다', 'Verb'), ('수', 'Noun'), ('있다', 'Adjective'), ('?', 'Punctuation')]\n",
            "['가', '를']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvMIo31fb2qQ",
        "outputId": "817285fd-17ad-44e4-9b71-f3107eeac3d8"
      },
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma=Kkma()\n",
        "print(kkma.morphs(\"우리가 이 과제를 잘 할 수 있을까?\"))\n",
        "print(kkma.pos(\"우리가 이 과제를 잘 할 수 있을까?\"))\n",
        "print(kkma.nouns(\"우리가 이 과제를 잘 할 수 있을까?\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['우리', '가', '이', '과제', '를', '잘', '하', 'ㄹ', '수', '있', '을까', '?']\n",
            "[('우리', 'NP'), ('가', 'JKS'), ('이', 'MDT'), ('과제', 'NNG'), ('를', 'JKO'), ('잘', 'MAG'), ('하', 'VV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('을까', 'EFQ'), ('?', 'SF')]\n",
            "['우리', '과제', '수']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}